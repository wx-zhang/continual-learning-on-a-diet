# disable hydra outputs
hydra:
  output_subdir: null
  run:
    dir: ./hydra-outputs
  
defaults:
  - _self_
  - dataset@_here_: imagenet10k
  - trainer@_here_: diet


seed: 0

# distributed training 
distributed: True
n_gpu_per_node: 2
nodes: 1
dist_backend: nccl
dist_url: tcp://127.0.0.1:50272
workers: 2

# logging 
wandb: 
  enable: True
  project: dietcl
base_dir: ./output
print_freq: 1
val_print_freq: 50
debug: False

# resume and evaluate
resume: /path/to/your/experiment_dir
evaluate_only: False
train_only: False


# training parameters provided by MAE
# augmentations
augment:
  mixup: 0.8
  cutmix: 1.0
  cutmix_minmax: 
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
  mixup_mode: batch
  label_smoothing: 0.1
input_size: 224
# optimization
layer_decay: 0.65
batch_size: 256 # batch size per device
base_lr: 1e-4
weight_decay: 0.05
warmup: 0
min_lr: 5e-5

# continual learning parameters
sampling: batchmix
replay_buffer_size: -1